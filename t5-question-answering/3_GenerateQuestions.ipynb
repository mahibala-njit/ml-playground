{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"65a2fa4fe9d6454db101a7675e3bdb9a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ded5e2f13d9408a8a4e5b8ccd555ee8","IPY_MODEL_5270590779e24aebabbcf7560a9942fd","IPY_MODEL_8f450c5d72fc4e66b7ec9be009a2b549"],"layout":"IPY_MODEL_8f5d88305bbc4c8d8fa8f0bed795517f"}},"0ded5e2f13d9408a8a4e5b8ccd555ee8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9697e067110b48c38d65109ef166693f","placeholder":"​","style":"IPY_MODEL_05d1a2f06ca64a4d8f8e1115a17697e5","value":"Downloading pytorch_model.bin: 100%"}},"5270590779e24aebabbcf7560a9942fd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1051ac9ed6744dcb320f564d69306fb","max":1187795641,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9cb55a4036ae4c3bac9665739a8cb61b","value":1187795641}},"8f450c5d72fc4e66b7ec9be009a2b549":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_190b26344c7a4c6781831a6e6161b4b5","placeholder":"​","style":"IPY_MODEL_5c58ce7d705643bcb6be8bfbcd2e2998","value":" 1.19G/1.19G [00:29&lt;00:00, 40.8MB/s]"}},"8f5d88305bbc4c8d8fa8f0bed795517f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9697e067110b48c38d65109ef166693f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05d1a2f06ca64a4d8f8e1115a17697e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1051ac9ed6744dcb320f564d69306fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9cb55a4036ae4c3bac9665739a8cb61b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"190b26344c7a4c6781831a6e6161b4b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c58ce7d705643bcb6be8bfbcd2e2998":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["**Generate Questions using T5 tuned for Question generation**\n","\n","**Model 1 :**  https://huggingface.co/mrm8488/t5-base-finetuned-question-generation-ap\n","\n","Citation:\n","@misc{mromero2021t5-base-finetuned-question-generation-ap,\n","  title={T5 (base) fine-tuned on SQUAD for QG via AP},\n","  author={Romero, Manuel},\n","  publisher={Hugging Face},\n","  journal={Hugging Face Hub},\n","  howpublished={\\url{https://huggingface.co/mrm8488/t5-base-finetuned-question-generation-ap}},\n","  year={2021}\n","}\n","\n","**FLAN-T5 is just given as reference but not used in the implementation. Future work.**\n","\n","\n","**Model 2:** https://huggingface.co/google/flan-t5-xxl\n","\n","@misc{https://doi.org/10.48550/arxiv.2210.11416,\n","  doi = {10.48550/ARXIV.2210.11416},\n","  \n","  url = {https://arxiv.org/abs/2210.11416},\n","  \n","  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\n","  \n","  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n","  \n","  title = {Scaling Instruction-Finetuned Language Models},\n","  \n","  publisher = {arXiv},\n","  \n","  year = {2022},\n","  \n","  copyright = {Creative Commons Attribution 4.0 International}\n","}"],"metadata":{"id":"Ha9eoMKNn4Sp"}},{"cell_type":"code","source":["!pip install transformers\n","!pip install sentencepiece\n","!pip install --upgrade transformers accelerate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QupJKcPUolu9","executionInfo":{"status":"ok","timestamp":1682397038621,"user_tz":240,"elapsed":36641,"user":{"displayName":"Mahalakshmi Balasubramanian","userId":"07488465586135634643"}},"outputId":"7f1e2d1e-e94a-4889-e765-fcd94a221a13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.14.0-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.2/224.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.0 tokenizers-0.13.3 transformers-4.28.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.98\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n","Collecting accelerate\n","  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.14.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (2.0.0+cu118)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (1.11.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (2.0.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (3.1.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (3.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (16.0.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (3.25.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.4.0->accelerate) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.4.0->accelerate) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.18.0\n"]}]},{"cell_type":"code","source":["# connect your personal google drive to store dataset and trained model\n","from google.colab import drive\n","drive.mount('/content/gdrive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oJjjc2-woEUb","executionInfo":{"status":"ok","timestamp":1681859517588,"user_tz":240,"elapsed":734,"user":{"displayName":"Mahalakshmi B","userId":"12994082256051428887"}},"outputId":"d71fc24c-85b1-4010-9bc5-7f84348f75d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"]}]},{"cell_type":"code","source":["from transformers import AutoModelWithLMHead, AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n","model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n","\n","def get_question(answer, context, max_length=64):\n","  input_text = \"answer: %s  context: %s </s>\" % (answer, context)\n","  features = tokenizer([input_text], return_tensors='pt')\n","\n","  output = model.generate(input_ids=features['input_ids'], \n","               attention_mask=features['attention_mask'],\n","               max_length=max_length)\n","\n","  return tokenizer.decode(output[0])\n","\n","get_question(answer, context)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":192,"referenced_widgets":["65a2fa4fe9d6454db101a7675e3bdb9a","0ded5e2f13d9408a8a4e5b8ccd555ee8","5270590779e24aebabbcf7560a9942fd","8f450c5d72fc4e66b7ec9be009a2b549","8f5d88305bbc4c8d8fa8f0bed795517f","9697e067110b48c38d65109ef166693f","05d1a2f06ca64a4d8f8e1115a17697e5","c1051ac9ed6744dcb320f564d69306fb","9cb55a4036ae4c3bac9665739a8cb61b","190b26344c7a4c6781831a6e6161b4b5","5c58ce7d705643bcb6be8bfbcd2e2998"]},"id":"I5mxuE57ocpa","executionInfo":{"status":"ok","timestamp":1681859559826,"user_tz":240,"elapsed":41354,"user":{"displayName":"Mahalakshmi B","userId":"12994082256051428887"}},"outputId":"004773df-50e8-4603-d979-a33b9f3cb867"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n","The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n","The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n","/usr/local/lib/python3.9/dist-packages/transformers/models/auto/modeling_auto.py:1322: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n","  warnings.warn(\n","The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65a2fa4fe9d6454db101a7675e3bdb9a"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["'<pad> question: Who created the RuPERTa-base?</s>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["def get_questions_for_source_t5(source):\n","  context_questions = []\n","  for index, row in source.iterrows():\n","    context = row['context'] \n","    answer = row['keyword']\n","    question = get_question(answer, context)\n","    new_tuple = (context,answer,question)\n","    context_questions.append(new_tuple)\n","  return context_questions"],"metadata":{"id":"FN3cIm24wful"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","source=pd.read_excel(\"/content/gdrive/My Drive/CS 677 Project/dataset/context_keywords.xlsx\", sheet_name='Sheet_name_1')"],"metadata":{"id":"SMDWhUgkr3Z0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["source.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"Cfij0-tJsxrF","executionInfo":{"status":"ok","timestamp":1681860544550,"user_tz":240,"elapsed":132,"user":{"displayName":"Mahalakshmi B","userId":"12994082256051428887"}},"outputId":"247406ab-dd40-439f-e72b-8602b84d26cb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Unnamed: 0            keyword  normalized_weight  \\\n","0           0     form molecules           0.879163   \n","1           1   chemical bonding           0.841418   \n","2           2    building blocks           0.772690   \n","3           3              Atoms           0.763432   \n","4           4  Christian Guthier           0.743297   \n","\n","                                             context  \n","0  Atoms are the building blocks that come togeth...  \n","1  Atoms are the building blocks that come togeth...  \n","2  Atoms are the building blocks that come togeth...  \n","3  Atoms are the building blocks that come togeth...  \n","4  Atoms are the building blocks that come togeth...  "],"text/html":["\n","  <div id=\"df-301f4262-f6d3-4832-a8fb-2b6de0ed2509\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>keyword</th>\n","      <th>normalized_weight</th>\n","      <th>context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>form molecules</td>\n","      <td>0.879163</td>\n","      <td>Atoms are the building blocks that come togeth...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>chemical bonding</td>\n","      <td>0.841418</td>\n","      <td>Atoms are the building blocks that come togeth...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>building blocks</td>\n","      <td>0.772690</td>\n","      <td>Atoms are the building blocks that come togeth...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Atoms</td>\n","      <td>0.763432</td>\n","      <td>Atoms are the building blocks that come togeth...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>Christian Guthier</td>\n","      <td>0.743297</td>\n","      <td>Atoms are the building blocks that come togeth...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-301f4262-f6d3-4832-a8fb-2b6de0ed2509')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-301f4262-f6d3-4832-a8fb-2b6de0ed2509 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-301f4262-f6d3-4832-a8fb-2b6de0ed2509');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["context_questions_tuple = get_questions_for_source_t5(source)\n","context_questions = pd.DataFrame(context_questions_tuple)\n","context_questions.to_excel(\"/content/gdrive/My Drive/CS 677 Project/dataset/context_questions.xlsx\", sheet_name='Sheet_name_1')  "],"metadata":{"id":"Y-mvijbLsjAd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example 1\n","context = \"Atoms are the building blocks that come together through chemical bonding to form molecules in the universe. In this model of a molecule, the atoms of carbon (black), hydrogen (white), nitrogen (blue), oxygen (red), and sulfur (yellow) are in proportional atomic size. The silver rods indicate chemical bonds that hold the atoms together in a specific three-dimensional shape. (credit: modification of work by Christian Guthier).\"\n","answer = \"form molecules\"\n","get_question(answer, context)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"3igT2dy1pU7w","executionInfo":{"status":"ok","timestamp":1681859793255,"user_tz":240,"elapsed":1741,"user":{"displayName":"Mahalakshmi B","userId":"12994082256051428887"}},"outputId":"ec9e6dfe-f113-4700-df1a-21fee9e06aca"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<pad> question: Atoms are the building blocks that come together through chemical bonding to do what in the universe?</s>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Example 2\n","context = \"Atoms are the building blocks that come together through chemical bonding to form molecules in the universe. In this model of a molecule, the atoms of carbon (black), hydrogen (white), nitrogen (blue), oxygen (red), and sulfur (yellow) are in proportional atomic size. The silver rods indicate chemical bonds that hold the atoms together in a specific three-dimensional shape. (credit: modification of work by Christian Guthier).\"\n","answer = \"building blocks\"\n","get_question(answer, context)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"hG3Gs7t0prSE","executionInfo":{"status":"ok","timestamp":1681859907807,"user_tz":240,"elapsed":1190,"user":{"displayName":"Mahalakshmi B","userId":"12994082256051428887"}},"outputId":"97ddf150-de84-48fc-f5c6-b2a819403602"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<pad> question: What are atoms?</s>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# Example 3\n","context = \"Atoms are the building blocks that come together through chemical bonding to form molecules in the universe. In this model of a molecule, the atoms of carbon (black), hydrogen (white), nitrogen (blue), oxygen (red), and sulfur (yellow) are in proportional atomic size. The silver rods indicate chemical bonds that hold the atoms together in a specific three-dimensional shape. (credit: modification of work by Christian Guthier).\"\n","answer = \"Atoms\"\n","get_question(answer, context)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Xjyu7xo1qRyx","executionInfo":{"status":"ok","timestamp":1681859912058,"user_tz":240,"elapsed":1514,"user":{"displayName":"Mahalakshmi B","userId":"12994082256051428887"}},"outputId":"65f8a2b7-bae2-4d04-b425-e29f92a1d603"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<pad> question: What are the building blocks that come together through chemical bonding to form molecules in the universe?</s>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Example 4\n","context = \"Atoms are the building blocks that come together through chemical bonding to form molecules in the universe. In this model of a molecule, the atoms of carbon (black), hydrogen (white), nitrogen (blue), oxygen (red), and sulfur (yellow) are in proportional atomic size. The silver rods indicate chemical bonds that hold the atoms together in a specific three-dimensional shape. (credit: modification of work by Christian Guthier).\"\n","answer = \"molecules\"\n","get_question(answer, context)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"3s08aM2uok_V","executionInfo":{"status":"ok","timestamp":1681860035629,"user_tz":240,"elapsed":1718,"user":{"displayName":"Mahalakshmi B","userId":"12994082256051428887"}},"outputId":"696a602f-3837-4701-9e13-78f4bb056062"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<pad> question: Atoms are the building blocks that come together through chemical bonding to form what in the universe?</s>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["#----------REFERENCE FOR FLAN-T5 -- NOT USED CURRENTLY -- FUTURE WORK ----------\n","# Generate Questions using flan-T5-xxl\n","\n","#!pip install accelerate\n","#from transformers import T5Tokenizer, T5ForConditionalGeneration\n","#from accelerate import init_empty_weights\n","#\n","#tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\")\n","#model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xxl\", device_map=\"auto\", offload_folder=\"/content/gdrive/My Drive/CS677Working/offload\")\n","#\n","#input_text = \"Generate Question: Atoms are the building blocks that come together through chemical bonding to form molecules in the universe. In this model of a molecule, the atoms of carbon (black), hydrogen (white), nitrogen (blue), oxygen (red), and sulfur (yellow) are in proportional atomic size. The silver rods indicate chemical bonds that hold the atoms together in a specific three-dimensional shape. (credit: modification of work by Christian Guthier).\"\n","#input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n","#\n","#outputs = model.generate(input_ids)\n","#print(tokenizer.decode(outputs[0]))"],"metadata":{"id":"uXEhRGFwwBbG"},"execution_count":null,"outputs":[]}]}